{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Person of Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Date:10/01/2018\n",
    "\n",
    "Author: Mamadou Diallo\n",
    "\n",
    "Email: mams.diallo@gmail.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- option 1: missing value: replace with zero [X]\n",
    "- option 2: missing value: replace with median\n",
    "- option 3: missing value: drop the feature [X]\n",
    "- Values for perc are above 1 [X] no pb\n",
    "- new features: continuous variable converted into discrete variables.\n",
    "- importance is not working it is only alphabetic order. [X] solved \n",
    "- Importance to be done with training dataset\n",
    "- check if overfitting or underfitting\n",
    "- add feature scaling [X] no since the selected methods are not affected\n",
    "- unbalanced data : use techniques\n",
    "- process is missing [X]\n",
    "- folder affects results\n",
    "- missing values: no decision\n",
    "- select only 3 models [X]\n",
    "- what performance measure to use. Justification: F1 is a tradeoff for Precision and Recall. Accuracy is bad since the dataset is unbalanced.\n",
    "- use of GridSearch: care with the parameters.\n",
    "- use of tasks to help reader.\n",
    "- in conclusion states what could be the use of such a system: tracking fraud, saving time and money/fines, better performance than audits, automate. Most of the time, frauds are detected after the facts by the press and when late. the system is also justified if it outperforms the human in detecting the fraud. Frauds are considered as high risks.\n",
    "- in the case of POI, do we favor precision or recall? expression:  100 culprits may let go free but no innocent should be punished. PRECISION matters most.\n",
    "- use pipeline []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check list <a class=\"anchor\" id=\"#third-bullet\"></a>\n",
    "i. Summarize for us the goal of this project and how machine learning is useful in trying to accomplish it. [x] \n",
    "\n",
    "ii. As part of your answer, give some background on the dataset and how it can be used to answer the project question. \n",
    "\n",
    "iii. Were there any outliers in the data when you got it, and how did you handle those?  [relevant rubric items: “data exploration”, “outlier investigation”] [X]\n",
    "\n",
    "What features did you end up using in your POI identifier, and what selection process did you use to pick them? [X]\n",
    "\n",
    "Did you have to do any scaling? Why or why not? [X]\n",
    "\n",
    "As part of the assignment, you should attempt to engineer your own feature that does not come ready-made in the dataset -- explain what feature you tried to make, and the rationale behind it. (You do not necessarily have to use it in the final analysis, only engineer and test it.) In your feature selection step, if you used an algorithm like a decision tree, please also give the feature importances of the features that you use, and if you used an automated feature selection function like SelectKBest, please report the feature scores and reasons for your choice of parameter values.  [relevant rubric items: “create new features”, “intelligently select features”, “properly scale features”] [X]\n",
    "\n",
    "What algorithm did you end up using? What other one(s) did you try? How did model performance differ between algorithms?  [relevant rubric item: “pick an algorithm”]\n",
    "\n",
    "What does it mean to tune the parameters of an algorithm, and what can happen if you don’t do this well?  How did you tune the parameters of your particular algorithm? What parameters did you tune? (Some algorithms do not have parameters that you need to tune -- if this is the case for the one you picked, identify and briefly explain how you would have done it for the model that was not your final choice or a different model that does utilize parameter tuning, e.g. a decision tree classifier).  [relevant rubric items: “discuss parameter tuning”, “tune the algorithm”]\n",
    "\n",
    "What is validation, and what’s a classic mistake you can make if you do it wrong? How did you validate your analysis?  [relevant rubric items: “discuss validation”, “validation strategy”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sections of this analysis include:\n",
    "- “data exploration”, “outlier investigation”\n",
    "\n",
    "- “create new features”, “intelligently select features”, “properly scale features”\n",
    "\n",
    "- “pick an algorithm”\n",
    "\n",
    "- “discuss parameter tuning”, “tune the algorithm”\n",
    "\n",
    "- “discuss validation”, “validation strategy”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Enron scandal, publicized in October 2001, eventually led to the bankruptcy of the Enron Corporation, an American energy company based in Houston, Texas, and the de facto dissolution of Arthur Andersen, which was one of the five largest audit and accountancy partnerships in the world. In addition to being the largest bankruptcy reorganization in American history at that time, Enron was cited as the biggest audit failure. (source: Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I'm building a person of interest identifier based on financial and email data made public as a result of the Enron scandal and with the help of machine learning techniques. I won't process the data emails_by_address."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning allows to predict poi feature. Feature 'poi' is the response variable and it takes value 1 in case of poi and 0 otherwise. A person of Interest is a person who might be involved in the fraud causing the bankruptcy of Enron. I'll use specifically supervised machine learning since we have a labeled dataset where we know whether or not each datapoint is poi or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will use dimension reduction for visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Techniques <a class=\"anchor\" id=\"#fifth-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Dataset and questions\n",
    "2. Features\n",
    "3. Algorithm\n",
    "input: final_project_dataset.pkl\n",
    "starter code: poi_id.py\n",
    "\n",
    "4. Evaluation\n",
    "tester.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset and Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Question\n",
    "- poi identifier\n",
    "- could we predict if a given person is a poi?\n",
    "- Could we predict fraud? [x] \n",
    "\n",
    "Approach\n",
    "- type of problem: supervised or semi-supervised? supervised since we have a labeled dataset where we know whether or not each datapoint is poi or not.\n",
    "- anomaly detection: what is it? how to proceed\n",
    "\n",
    "Data\n",
    "- source: financial data, email data, poi list, emails_by_address\n",
    "- do I have enough data? -> vague\n",
    "- what feature to select?\n",
    "- how do we deal with missing data?\n",
    "- is the data unbalanced? \n",
    "- if so, how do we deal with unbalanced data?\n",
    "- do we build new features?\n",
    "\n",
    "Approach\n",
    "- do we need dimension reduction? preferable\n",
    "- I don't process the data emails_by_address\n",
    "\n",
    "- do we apply feature scaling? \n",
    "Yes, It’s a type of feature preprocessing that we should perform before some classification and regression tasks\n",
    "\n",
    "Evaluation\n",
    "- What about false positive?\n",
    "- what mesures to use?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How to answer the questions\n",
    "- techniques and algorithm for anomaly detection from [Semi-supervised learning]\n",
    "- unbalance data -> resampling\n",
    "- approach unsupervised\n",
    "- feature selection: technique -> boost\n",
    "- missing data: Measure the % of missing values per feature (i.e. variable). Use of articles on the subject such as document [MV]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dataset Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Dependant variables: \n",
    "bonus\n",
    "deferral_payments\n",
    "deferred_income\n",
    "director_fees\n",
    "email_address\n",
    "exercised_stock_options\n",
    "expenses\n",
    "from_messages\n",
    "from_poi_to_this_person\n",
    "from_this_person_to_poi\n",
    "loan_advances\n",
    "long_term_incentive\n",
    "other\n",
    "poi\n",
    "restricted_stock\n",
    "restricted_stock_deferred\n",
    "salary\n",
    "shared_receipt_with_poi\n",
    "to_messages\n",
    "total_payments\n",
    "total_stock_value\n",
    "\n",
    "- Independant variable: \n",
    "poi\n",
    "\n",
    "Some people pleaded guilty -> we know they is fraud\n",
    "\n",
    "How many poi in the dataset: 18\n",
    "\n",
    "the data is unbalanced: fraud is rare\n",
    "Consequently, measures have to be adapted: Given the class imbalance ratio, we recommend measuring the accuracy using the Area Under the Precision-Recall Curve (AUPRC). Confusion matrix accuracy is not meaningful for unbalanced classification.\n",
    "\n",
    "Anomaly detection:\n",
    "\"Anomalies are patterns in data that do not conform to a well defined notion of normal behavior\" (Chandola et al, 2009). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Orginal features\n",
    "\n",
    "-|Feature  | Type | Comment\n",
    "-|--|--|--\n",
    "1|bonus|continuous| finance (payment type) feature\n",
    "2|deferral_payments|continuous| finance (payment type) feature\n",
    "3|deferred_income|continuous|finance (payment type) feature\n",
    "4|director_fees|continuous|finance (payment type) feature\n",
    "5|email_address|nominal|email (text type) feature \n",
    "6|exercised_stock_options|continuous|finance (stock type) feature\n",
    "7|expenses|continuous|finance (payment type) feature\n",
    "8|from_messages|nominal|email (number of messages) feature \n",
    "9|from_poi_to_this_person|continuous|email (number of messages) feature\n",
    "10|from_this_person_to_poi|continuous|email (number of messages) feature\n",
    "11|loan_advances|continuous|finance (payment type) feature\n",
    "12|long_term_incentive|continuous|finance (payment type) feature\n",
    "13|other|continuous|finance (payment type) feature\n",
    "14|poi|nominal|the label to identify a person of interest (boolean type)\n",
    "15|restricted_stock|continuous|finance (stock type) feature\n",
    "16|restricted_stock_deferred|continuous|finance (stock type) feature\n",
    "17|salary|continuous|finance (payment type) feature\n",
    "18|shared_receipt_with_poi|continuous|email (number of messages) feature\n",
    "19|to_messages|continuous|email (number of messages) feature\n",
    "20|total_payments|continuous|finance (payment type) feature\n",
    "21|total_stock_value|continuous|finance (stock type) feature\n",
    "\n",
    "Number of observations: 146\n",
    "\n",
    "How many poi in the dataset: 18\n",
    "\n",
    "There are two types of revenues: stocks and payments\n",
    "\n",
    "We will not exploit the feature 'email_address' for obvious reasons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Missing data (after outlier removal):\n",
    "When NaN is filled\n",
    "\n",
    "![Diagram](MissingDataAmongFeatures.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Were there any outliers in the data when you got it, and how did you handle those?\n",
    "[relevant rubric items: “data exploration”, “outlier investigation”]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Outlier Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original question: Were there any outliers in the data when you got it, and how did you handle those? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "how to answer the question: technique Outlier removal strategy/Outlier rejection\n",
    "Train - Remove the points with largest residual error - Re-train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers are ways to identifier fraud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the graph \"Bonus vs Salary\". A clear outlier is appearing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagram](Outlier_TOTAL.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outlier is caught by looking for the highest bonus and highest salary. \n",
    "\n",
    "The salary is 26,704,229\n",
    "\n",
    "The bonus is 97,343,619\n",
    "\n",
    "The name \"TOTAL\" is a hint that it is not a person but the sum of financial features. \n",
    "moreover with this outlier the pearsonr is 0.99. It is very suspicious.\n",
    "\n",
    "It has to be discarded. \n",
    "\n",
    "After removal, we have a more acceptatble pearsonr and the following graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagram](Bonus_versus_Salary_AfterTOTALremoval.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Missing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of missing data is significant and we cannot discard the data since we have only 145 rows\n",
    "!!!!! Assumption: if nan -> replace with zeros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the following people we have less than 90% of information:\n",
    "\n",
    "['WHALEY DAVID A', 'WROBEL BRUCE', 'LOCKHART EUGENE E', 'THE TRAVEL AGENCY IN THE PARK', 'GRAMM WENDY L']\n",
    "\n",
    "'THE TRAVEL AGENCY IN THE PARK' is obviously not a person. \n",
    "\n",
    "They will be discarded from the dataset since they do not bring information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection and Engineering <a class=\"anchor\" id=\"#second-bullet\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new email features:\n",
    "\n",
    "given a number messages to/from POI (numerator) and number of all messages to/from a person (denominator), return the fraction of messages to/from that person that are from/to a POI\n",
    "\n",
    "The second set of engineered features relates to the ratio (r stands for ratio) of email sent to or received from a POI. Since the total data for to and from is available, getting the ratio is rather easy.\n",
    "\n",
    "These features will reveal the persons with higher communications with POI in a percentage basis. The reason behind it is because it could be the case that somebody went unnoticed with lower email volume, but almost all of it directed or from POI. This new feature would help surface these edge cases\n",
    "\n",
    "    fraction = 0.\n",
    "\n",
    "    #print \"poi_messages\",poi_messages\n",
    "    #print \"all_messages\",all_messages\n",
    "    import math\n",
    "    \n",
    "    if poi_messages != \"NaN\" and all_messages != \"NaN\":\n",
    "        fraction = float(poi_messages)/float(all_messages)\n",
    "\n",
    "    return fraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create new email features:\n",
    "\n",
    "New financial features\n",
    "\n",
    "perc_bonus = bonus / total_payments\n",
    "perc_salary = salary / total_payments\n",
    "perc_stock = total_stock_value / total_payments\n",
    "\n",
    "The first set of engineered features relates to the fraction (f stands for fraction) from the type of financial incentives received. Employees usually can be rewarded mainly through three mechanisms: salary, bonus or stock.\n",
    "\n",
    "The goal here is to understand if higher fractions of payments in certain modalities led to POI. Say, for example, all the involved persons in the scandal were to be payed mostly through bonuses. Having the fraction of each type of payment could potentially spot the ones under such circumstances.\n",
    "\n",
    "Since all three sources are available and have little missing values, they seemed like an interesting choice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "univariate feature selection tools in sklearn: SelectPercentile and SelectKBest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Did you have to do any scaling? Why or why not?\n",
    "\n",
    "Decision tree and linear regression are not affected by feature scaling.  Naive Bayes, as well as Decision trees and Tree-based ensemble methods (RF, XGB) are invariant to feature scaling.\n",
    "Thus, I did not used feature scaling. \n",
    "Affected algorithms: SVM, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagram](FeatureImportance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick 1 feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature: exercised_stock_options\n",
    "    \n",
    "('est: RandomForestClassifier:', 10)\n",
    "('split: RandomForestClassifier:', 2)\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "            min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
    "            n_estimators=10, n_jobs=1, oob_score=False, random_state=42,\n",
    "            verbose=0, warm_start=False)\n",
    "        Accuracy: 0.88170       Precision: 0.38718      Recall: 0.31400 F1: 0.34677     F2: 0.32634\n",
    "        Total predictions: 10000        True positives:  314    False positives:  497   False negatives:  686   True negatives: 8503\n",
    "\n",
    "Random Forest time: 36.177 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pick 2 features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "'exercised_stock_options','bonus'\n",
    "\n",
    "('est: RandomForestClassifier:', 10)\n",
    "('split: RandomForestClassifier:', 2)\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "            max_depth=None, max_features=None, max_leaf_nodes=None,\n",
    "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "            min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
    "            n_estimators=10, n_jobs=1, oob_score=False, random_state=42,\n",
    "            verbose=0, warm_start=False)\n",
    "        Accuracy: 0.84333       Precision: 0.54894      Recall: 0.33650 F1: 0.41723     F2: 0.36473\n",
    "        Total predictions: 12000        True positives:  673    False positives:  553   False negatives: 1327   True negatives: 9447\n",
    "\n",
    "Random Forest time: 36.506 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick (6 selected features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get rid of correlated features. \n",
    "Features: 'exercised_stock_options','bonus','perc_bonus','salary','perc_to_poi','deferred_income'\n",
    "\n",
    "- Logistic regression: underfit with fewer features. A more powerful model is needed.\n",
    "\n",
    "- DecisionTreeClassifier\n",
    "\n",
    "-------------------------------------------\n",
    "('split range: Decision tree', 3)\n",
    "('depth range: Decision tree', 2)\n",
    "-------------------------------------------\n",
    "Accuracy train Decision Tree 0.908333333333\n",
    "Accuracy test Decision Tree 0.785714285714\n",
    "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
    "            max_features='log2', max_leaf_nodes=None,\n",
    "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "            min_samples_split=3, min_weight_fraction_leaf=0.0,\n",
    "            presort=False, random_state=42, splitter='best')\n",
    "        Accuracy: 0.84686       Precision: 0.45213      Recall: 0.34000 F1: 0.38813     F2: 0.35774\n",
    "        Total predictions: 14000        True positives:  680    False positives:  824   False negatives: 1320   True negatives: 11176\n",
    "\n",
    "Decision tree algorithm time: 0.737 s\n",
    "\n",
    "![Diagram](DecisionTree6Features.png)\n",
    "\n",
    "- Gaussian\n",
    "\n",
    "GaussianNB(priors=None)\n",
    "        Accuracy: 0.85950       Precision: 0.51220      Recall: 0.34650 F1: 0.41336     F2: 0.37047\n",
    "        Total predictions: 14000        True positives:  693    False positives:  660   False negatives: 1307   True negatives: 11340\n",
    "\n",
    "NB algorithm time: 1.086 s\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random Forest\n",
    "\n",
    "-------------------------------------------\n",
    "('n_estimators Random forest: ', 1)\n",
    "('criterion Random forest: ', 'entropy')\n",
    "('max_depth Random forest: ', 6)\n",
    "('max_features Random forest: ', 5)\n",
    "('split Random forest', 2)\n",
    "-------------------------------------------\n",
    "Accuracy train random forest 0.933333333333\n",
    "Accuracy test random forest 0.785714285714\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "            max_depth=6, max_features=5, max_leaf_nodes=None,\n",
    "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "            n_estimators=1, n_jobs=1, oob_score=False, random_state=42,\n",
    "            verbose=0, warm_start=False)\n",
    "        Accuracy: 0.80893       Precision: 0.32558      Recall: 0.31500 F1: 0.32020     F2: 0.31706\n",
    "        Total predictions: 14000        True positives:  630    False positives: 1305   False negatives: 1370   True negatives: 10695\n",
    "\n",
    "Random Forest time: 4.842 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick an algorithm (7 selected features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decision Tree\n",
    "-------------------------------------------\n",
    "('split range: Decision tree', 3)\n",
    "('depth range: Decision tree', 2)\n",
    "-------------------------------------------\n",
    "Accuracy train Decision Tree 0.894308943089\n",
    "Accuracy test Decision Tree 0.928571428571\n",
    "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
    "            max_features='log2', max_leaf_nodes=None,\n",
    "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "            min_samples_split=3, min_weight_fraction_leaf=0.0,\n",
    "            presort=False, random_state=42, splitter='best')\n",
    "        Accuracy: 0.85321       Precision: 0.48118      Recall: 0.35150 F1: 0.40624     F2: 0.37153\n",
    "        Total predictions: 14000        True positives:  703    False positives:  758   False negatives: 1297   True negatives: 11242\n",
    "\n",
    "Decision tree algorithm time: 0.775 s\n",
    "\n",
    "![Diagram](DecisionTree7features.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random forest.\n",
    "('n_estimators Random forest: ', 1)\n",
    "('criterion Random forest: ', 'entropy')\n",
    "('max_depth Random forest: ', 7)\n",
    "('split Random forest', 2)\n",
    "('max_features Random forest: ', 7)\n",
    "-------------------------------------------\n",
    "Accuracy train random forest 0.934959349593\n",
    "Accuracy test random forest 0.928571428571\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "            max_depth=7, max_features=7, max_leaf_nodes=None,\n",
    "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "            n_estimators=1, n_jobs=1, oob_score=False, random_state=42,\n",
    "            verbose=0, warm_start=False)\n",
    "        Accuracy: 0.81150       Precision: 0.32813      Recall: 0.30500 F1: 0.31614     F2: 0.30936\n",
    "        Total predictions: 14000        True positives:  610    False positives: 1249   False negatives: 1390   True negatives: 10751\n",
    "\n",
    "Random Forest time: 4.92 s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pick an algorithm (all features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Logistic Regression. We have a classification problem. It is the most obvious classification algorithm to try. Pros: easy to understand. Cons: to simple to capture complexities between variables. tendencies of the model to overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decision tree. Pros: easy to understand and to implement. Cons: limitation for complex data. Parameters: min_samples_split=split,max_depth=rg. Looping over t_split_range = [2,3,4,5,6,7,8,9,10,20,30] and max_depth_range = [2,3,4,5,6,7,8,9,10,20,30]\n",
    "\n",
    "Accuracy train Decision Tree 0.904\n",
    "Accuracy test Decision Tree 0.857142857143\n",
    "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=2,\n",
    "            max_features='log2', max_leaf_nodes=None,\n",
    "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "            min_samples_split=3, min_weight_fraction_leaf=0.0,\n",
    "            presort=False, random_state=42, splitter='best')\n",
    "Accuracy: 0.84207       Precision: 0.42962      Recall: 0.32200 F1: 0.36811     F2: 0.33898\n",
    "        \n",
    "![Diagram](DecisionTree.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SVM. Outcome = Poor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Random forest. Pros: Fast to train. \n",
    "n_estimator, max_depth, max_features, min_sample_split\n",
    "1,5,4,3\n",
    "1,5,10,2\n",
    "1,5,16,6\n",
    "1,5,20,4\n",
    "1,5,20,10\n",
    "1,7,4,4\n",
    "1,7,16,6\n",
    "\n",
    "-------------------------------------------\n",
    "('n_estimators Random forest: ', 1)\n",
    "('criterion Random forest: ', 'entropy')\n",
    "('max_depth Random forest: ', 7)\n",
    "('split Random forest', 2)\n",
    "('max_features Random forest: ', 3)\n",
    "-------------------------------------------\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "            max_depth=7, max_features=3, max_leaf_nodes=None,\n",
    "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "            n_estimators=1, n_jobs=1, oob_score=False, random_state=42,\n",
    "            verbose=0, warm_start=False)\n",
    "        Accuracy: 0.81350       Precision: 0.33224      Recall: 0.30250 F1: 0.31667     F2: 0.30801\n",
    "        Total predictions: 14000        True positives:  605    False positives: 1216   False negatives: 1395   True negatives: 10784\n",
    "        \n",
    "\n",
    "('n_estimators Random forest: ', 1)\n",
    "('criterion Random forest: ', 'entropy')\n",
    "('max_depth Random forest: ', 6)\n",
    "('split Random forest', 2)\n",
    "('max_features Random forest: ', 3)\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "            max_depth=6, max_features=3, max_leaf_nodes=None,\n",
    "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "            n_estimators=1, n_jobs=1, oob_score=False, random_state=42,\n",
    "            verbose=0, warm_start=False)\n",
    "        Accuracy: 0.81650       Precision: 0.33752      Recall: 0.29550 F1: 0.31512     F2: 0.30305\n",
    "        Total predictions: 14000        True positives:  591    False positives: 1160   False negatives: 1409   True negatives: 10840\n",
    "        \n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
    "            max_depth=8, max_features='log2', max_leaf_nodes=None,\n",
    "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "            min_samples_split=8, min_weight_fraction_leaf=0.0,\n",
    "            n_estimators=10, n_jobs=1, oob_score=False, random_state=42,\n",
    "            verbose=0, warm_start=False)\n",
    "        Accuracy: 0.85207       Precision: 0.45235      Recall: 0.16850 F1: 0.24554     F2: 0.19268\n",
    "        Total predictions: 14000        True positives:  337    False positives:  408   False negatives: 1663   True negatives: 11592\n",
    "\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "            max_depth=9, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "            n_estimators=5, n_jobs=1, oob_score=False, random_state=42,\n",
    "            verbose=0, warm_start=False)\n",
    "        Accuracy: 0.84343       Precision: 0.40144      Recall: 0.19550 F1: 0.26295     F2: 0.21785\n",
    "        Total predictions: 14000        True positives:  391    False positives:  583   False negatives: 1609   True negatives: 11417\n",
    "\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "            max_depth=6, max_features='auto', max_leaf_nodes=None,\n",
    "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "            min_samples_split=2, min_weight_fraction_leaf=0.0,\n",
    "            n_estimators=3, n_jobs=1, oob_score=False, random_state=42,\n",
    "            verbose=0, warm_start=False)\n",
    "        Accuracy: 0.83236       Precision: 0.35790      Recall: 0.21850 F1: 0.27134     F2: 0.23696\n",
    "        Total predictions: 14000        True positives:  437    False positives:  784   False negatives: 1563   True negatives: 11216\n",
    "\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "            max_depth=2, max_features=14, max_leaf_nodes=None,\n",
    "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "            min_samples_split=9, min_weight_fraction_leaf=0.0,\n",
    "            n_estimators=1, n_jobs=1, oob_score=False, random_state=42,\n",
    "            verbose=0, warm_start=False)\n",
    "        Accuracy: 0.81871       Precision: 0.31946      Recall: 0.23800 F1: 0.27278     F2: 0.25079\n",
    "        Total predictions: 14000        True positives:  476    False positives: 1014   False negatives: 1524   True negatives: 10986\n",
    "\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "            max_depth=2, max_features=22, max_leaf_nodes=None,\n",
    "            min_impurity_split=1e-07, min_samples_leaf=1,\n",
    "            min_samples_split=10, min_weight_fraction_leaf=0.0,\n",
    "            n_estimators=1, n_jobs=1, oob_score=False, random_state=42,\n",
    "            verbose=0, warm_start=False)\n",
    "        Accuracy: 0.81700       Precision: 0.32547      Recall: 0.26200 F1: 0.29030     F2: 0.27263\n",
    "        Total predictions: 14000        True positives:  524    False positives: 1086   False negatives: 1476   True negatives: 10914\n",
    "        \n",
    "- Gradient boosting. Pros: High performing. Cons: A small change in the features set or in the training set can create a radical change in the model. Not easy to understand predictions.\n",
    "        \n",
    "- Neural network. Pros: can handle extremely complex tasks. Cons: very, very slow to train. Almost impossible to understand predictions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discuss parameter tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning techniques:\n",
    "1. try a smaller set of features: technique PCA\n",
    "Good when overfitting\n",
    "technique use of selectkbest:\n",
    "[X] worked with DecisionTreeClassifier: better results\n",
    "\n",
    "2. try getting additional features: was not helpful\n",
    "3. try increasing and decreasing λ (Change how important the regularization term). It was helpful for logistic regression [X]\n",
    "\n",
    "There are many things you can do:\n",
    "1. Get more training data\n",
    "\n",
    "    1.1 Sometimes more data doesn't help\n",
    "\n",
    "    1.2 Often it does though, although you should always do some preliminary testing to make sure more data will actually make a difference (discussed later)\n",
    "\n",
    "2. Try a smaller set a features\n",
    "\n",
    "    2.1 Carefully select small subset\n",
    "\n",
    "    2.2 You can do this by hand, or use some dimensionality reduction technique (e.g. PCA - we'll get to this later)\n",
    "    \n",
    "3. Try getting additional features\n",
    "\n",
    "    3.1 Sometimes this isn't helpful \n",
    "    \n",
    "    3.2 LOOK at the data\n",
    "    \n",
    "    3.3 Can be very time consuming\n",
    "    \n",
    "    \n",
    "4. Adding polynomial features\n",
    "\n",
    "    You're grasping at straws, aren't you...\n",
    "\n",
    "5. Building your own, new, better features based on your knowledge of the problem\n",
    "    \n",
    "    Can be risky if you accidentally over fit your data by creating new features which are inherently specific/relevant to your training data\n",
    "\n",
    "6. Try decreasing or increasing λ\n",
    "    \n",
    "    Change how important the regularization term is in your calculations\n",
    "    \n",
    "    [x] Done for LogisticRegression -> it works"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What are the evaluation measures?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagram](poiHistogram.jpg)\n",
    "\n",
    "in the Course, it is said that : Lesson 15 Evaluation metrics 3.\n",
    "Accuracy is defined by: number of items labelled correctly/all items in that class\n",
    "The accuracy should actually be no. of all data points labeled correctly divided by all data points. If we are looking at performance on a specific class, then we are looking at recall\n",
    "\n",
    "Shorcomings: \n",
    "not ideal for skewed classes\n",
    "may want to err on side of guessing innocent\n",
    "may want to err on side of guessing guilty\n",
    "\n",
    "\n",
    "Clearly the data is unbalanced\n",
    "Error metrics for skewed analysis have to be selected and we have to come up with something different from accuracy metric since it is not a proper one. Predicting poi=False is too frequent. \n",
    "\n",
    "Two new metrics - precision and recall - are good candidates\n",
    "Precision defines how often does our algorithm cause a false alarm?\n",
    "Of all persons we predicted to be person of interest, what fraction of them actually are person of interest\n",
    "\n",
    "Recall defines how sensitive is our algorithm?\n",
    "Of all persons in set that actually are person of interest, what fraction did we correctly detect\n",
    "\n",
    "For this application we want to control the trade-off between precision and recall\n",
    "\n",
    "We want to predict 1 only if very confident. we'll have a risk of false negatives: we guessed not poi but it was a poi.\n",
    "We want a high precision and the consequence will be a lower recall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "source: https://www.kaggle.com/shelars1985/anomaly-detection-using-gaussian-distribution\n",
    "If I can summarize what Andrew Ng has mentioned in his lecture on Anomaly detection is Supervised Classification technique is not the perfect candidate for highly imbalanced data. In this case it is 0.172% (near to 0)\n",
    "\n",
    "The dataset is fairly small. And here data matters most than algorithm.\n",
    "outliers have been removed (e.g. TOTAL) with significant impact.\n",
    "New features have been added.\n",
    "Relevant data have been used. When using all the data, models performed poorly.\n",
    "There was no way to have more training data\n",
    "The key influencing feature are financial ones:exercised_stock_options\n",
    "\n",
    "Difficulties:\n",
    "Some features have rare information: loan_advances, director_fees, restricted_stock_deferred\n",
    "How to tackle: do not include them in feature selection.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation/References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keywords: Fraud, email, financial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[kaggle competition] https://www.kaggle.com/wcukierski/enron-email-dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Semi-supervised learning] https://www.kaggle.com/matheusfacure/semi-supervised-anomaly-detection-survey"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Anomaly Detection Article 1] https://iwringer.wordpress.com/2015/11/17/anomaly-detection-concepts-and-techniques/\n",
    "\n",
    "[Anomaly Detection Article 2] https://www.datascience.com/blog/python-anomaly-detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[repo]\n",
    "https://github.com/DariaAlekseeva/Enron_Dataset\n",
    "http://nbviewer.jupyter.org/github/DariaAlekseeva/Enron_Dataset/blob/master/Enron%20POI%20Detector%20Project%20Assignment.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[repo]\n",
    "https://github.com/MarcCollado/Enron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[repo]\n",
    "https://github.com/travisseal/enron_data_udacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[repo]\n",
    "https://github.com/Jacobdudee/EnronModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[repo]\n",
    "https://github.com/adazamora/enron_ml/blob/master/ml_project.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[MV] How to treat missing values in you data from Article:\n",
    "[Article](https://clevertap.com/blog/how-to-treat-missing-values-in-your-data-part-i/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[example] https://towardsdatascience.com/building-a-logistic-regression-in-python-step-by-step-becd4d56c9c8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "[course Machine Learning] http://www.holehouse.org/mlclass/11_Machine_Learning_System_Design.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[course Machine Learning] http://www.holehouse.org/mlclass/10_Advice_for_applying_machine_learning.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "anomaly-detection-guidebook.pdf from Dataiku"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Hdbk] Python for Data Science Handbook from Blog:\n",
    "[blog](http://www.datasciencecentral.com/profiles/blogs/book-python-data-science-handbook?utm_content=buffer09a5c&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "https://github.com/ageron/handson-ml"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
