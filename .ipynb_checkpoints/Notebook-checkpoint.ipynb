{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Identify Person of Interest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Enron scandal, publicized in October 2001, eventually led to the bankruptcy of the Enron Corporation, an American energy company based in Houston, Texas, and the de facto dissolution of Arthur Andersen, which was one of the five largest audit and accountancy partnerships in the world. In addition to being the largest bankruptcy reorganization in American history at that time, Enron was cited as the biggest audit failure. (source: Wikipedia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this project, I'm building a person of interest identifier based on financial and email data made public as a result of the Enron scandal and with the help of machine learning techniques. I won't process the data emails_by_address."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine Learning allows to predict poi feature. Feature 'poi' is the response variable and it takes value 1 in case of poi and 0 otherwise. A person of Interest is a person who might be involved in the fraud causing the bankruptcy of Enron. This is a classification task. I'll use specifically supervised machine learning since we have a labeled dataset where we know whether or not each datapoint is poi or not."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source of the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The source file enron61702insiderpay.pdf provides the financial data and could be found in this github repository.\n",
    "\n",
    "The Enron email corpus provide the email data. It is not exploited in this study. \n",
    "\n",
    "There are missing values in the dataset. In this case, for a given person if the value of a given feature is missing its value is set to \"NaN\". When the data is transformed into a numpy array, \"NaN\" is converted to 0 by default."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Required Libraries\n",
    "import sys\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import string\n",
    "sys.path.append(\"./tools/\")\n",
    "from feature_format import featureFormat, targetFeatureSplit\n",
    "from tester import dump_classifier_and_data, test_classifier\n",
    "from explore import get_incompletes, display_examples,build_df\n",
    "from select_features import Select_k_best\n",
    "from create_new_features import add_fraction_to_dict,create_new_features,drop_features\n",
    "#from sklearn.cross_validation import train_test_split for previous versions\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Versions:\n",
    "Package|version\n",
    "--|--\n",
    "python|2.7.13\n",
    "scikit-learn|0.18.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Study"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Select what features you'll use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will not exploit the feature 'email_address'."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-|Feature  | Type | Comment\n",
    "-|--|--|--\n",
    "1|bonus|continuous| finance (payment type) feature\n",
    "2|deferral_payments|continuous| finance (payment type) feature\n",
    "3|deferred_income|continuous|finance (payment type) feature\n",
    "4|director_fees|continuous|finance (payment type) feature\n",
    "5|email_address|nominal|__NOT USED: email (text type) feature__\n",
    "6|exercised_stock_options|continuous|finance (stock type) feature\n",
    "7|expenses|continuous|finance (payment type) feature\n",
    "8|from_messages|nominal|email (number of messages) feature \n",
    "9|from_poi_to_this_person|continuous|email (number of messages) feature\n",
    "10|from_this_person_to_poi|continuous|email (number of messages) feature\n",
    "11|loan_advances|continuous|finance (payment type) feature\n",
    "12|long_term_incentive|continuous|finance (payment type) feature\n",
    "13|other|continuous|finance (payment type) feature\n",
    "14|poi|nominal|the label to identify a person of interest (boolean type)\n",
    "15|restricted_stock|continuous|finance (stock type) feature\n",
    "16|restricted_stock_deferred|continuous|finance (stock type) feature\n",
    "17|salary|continuous|finance (payment type) feature\n",
    "18|shared_receipt_with_poi|continuous|email (number of messages) feature\n",
    "19|to_messages|continuous|email (number of messages) feature\n",
    "20|total_payments|continuous|finance (payment type) feature\n",
    "21|total_stock_value|continuous|finance (stock type) feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a summary of the data: \n",
    "\n",
    "There are initially 146 people in this dataset.\n",
    "\n",
    "There are initially 21 features per person. poi feature is the target.\n",
    "\n",
    "We have a wealth of features but not so much data points.\n",
    "\n",
    "There are 18 persons of interest are there in this dataset. It represents 12.3% of the overall population.\n",
    "\n",
    "Machine learning algorithms work best when the classes are balanced - close to 50% - but the dataset in our hands is unbalanced in the distribution of the classes. We need to keep the same percentage of classes among datasets - training and testing - we can use StratifiedShuffleSplit. Another challenge is to find the proper metrics for performance evaluation in case of inbalance in classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### features_list is a list of strings, each of which is a feature name.\n",
    "### The first feature must be \"poi\" since it is the target.\n",
    "features_list = ['poi',\n",
    "                'bonus',\n",
    "                'deferral_payments',\n",
    "                'deferred_income',\n",
    "                'director_fees',\n",
    "                'exercised_stock_options',\n",
    "                'expenses',\n",
    "                'from_messages',\n",
    "                'from_poi_to_this_person',\n",
    "                'from_this_person_to_poi',\n",
    "                'loan_advances',\n",
    "                'long_term_incentive',\n",
    "                'other',\n",
    "                'restricted_stock',\n",
    "                'restricted_stock_deferred',\n",
    "                'salary',\n",
    "                'shared_receipt_with_poi',\n",
    "                'to_messages',\n",
    "                'total_payments',\n",
    "                'total_stock_value'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Load the dictionary containing the dataset\n",
    "with open(\"final_project_dataset.pkl\", \"r\") as data_file:\n",
    "    data_dict = pickle.load(data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We could consider that if a person did not receive any financial retribution, its value is zero.\n",
    "\n",
    "My choice is that NaN values have to be replaced by zeros.\n",
    "\n",
    "For the same reason, for email features, NaN values have to be replaced by zeros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here below are the top 5 features with missing value:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-|feature name  | % of missing information (NaN)| nb of non null values\n",
    "-|--|--|--\n",
    "1|loan_advances|97.92| 3\n",
    "2|director_fees|88.89| 16\n",
    "3|restricted_stock_deferred|88.19| 17\n",
    "4|deferral_payments|73.61| 38\n",
    "5|deferred_income|66.67| 48"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These features have not enough information. For this reason, I do not include the first 4 in feature selection. Here is the final discarded features:\n",
    "- loan_advances\n",
    "- restricted_stock_deferred\n",
    "- director_fees\n",
    "- deferral_payments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagram](PearsonCorrelationOfFeatures.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Linear Correlation deduced from Pearson's Correlation Diagram:__\n",
    "\n",
    "shared_receipt_with_poi and to_messages have a strong linear relationship (increasing).\n",
    "\n",
    "exercised_stock_options and total_stock_value have a strong linear relationship (increasing). It is not a surprise. \n",
    "\n",
    "Others and total_payment have a strong linear relationship (increasing).\n",
    "\n",
    "Exercised_stock_options and Restricted stock have a linear relationship (increasing)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Remove outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outlier Investigation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we get the list of persons with more than 90% incomplete information, we have the following table."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "-|name  | % of incomplete information | poi | bonus| salary|Comment\n",
    "-|--|--|--\n",
    "1|WHALEY DAVID A|90.48| false |NaN|NaN|\n",
    "2|WROBEL BRUCE|90.48| false |NaN|NaN|\n",
    "3|LOCKHART EUGENE E|100.0| false |NaN|NaN|no information\n",
    "4|THE TRAVEL AGENCY IN THE PARK|90.48| false |NaN|NaN|not a person obviously\n",
    "5|GRAMM WENDY L|90.48| false |NaN|NaN|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The person named \"LOCKHART EUGENE E\" is not an outlier per se but has no information. It has to be removed from the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The person named \"THE TRAVEL AGENCY IN THE PARK\" not a person and many missing information above 90%. It is clearly not an individual and very few information is gained from this person. It has to be discarded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When exploring the data, we can find that a person named \"TOTAL\" has the highest bonus and the highest salary with the following values:\n",
    "\n",
    "bonus  =  97,343,619 $\n",
    "\n",
    "salary = 26,704,229 $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's display the graph \"Bonus vs Salary\". A clear outlier is appearing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagram](Outlier_TOTAL.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The outlier \"TOTAL\" is caught by looking for the highest bonus and highest salary. \n",
    "\n",
    "poi value is 0\n",
    "\n",
    "The name \"TOTAL\" is a hint that it is not a person but the sum of financial features. \n",
    "\n",
    "It has to be discarded. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At last, here are the discarded outliers and assimilated :\n",
    "- TOTAL\n",
    "- THE TRAVEL AGENCY IN THE PARK\n",
    "- LOCKHART EUGENE E"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# removing the outlier called TOTAL\n",
    "data_dict.pop( \"TOTAL\", 0 );\n",
    "# not a person and many missing information above 90%\n",
    "data_dict.pop( \"THE TRAVEL AGENCY IN THE PARK\", 0 );\n",
    "# no information at all for LOCKHART EUGENE E\n",
    "data_dict.pop( \"LOCKHART EUGENE E\",0);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = build_df(data_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After removal, we have the following graph:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "![Diagram](bonusVSsalary.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "total number of data points 143\n",
    "\n",
    "number of poi:\n",
    "False    125\n",
    "True      18\n",
    "Name: poi, dtype: int64\n",
    "\n",
    "Percentage of POI  12.5874125874 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Create new feature(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direction:\n",
    "Create new feature(s)\n",
    "\n",
    "The two created features are related to ratios of emails sent to or from POI. It shows for that person what is the proportion of emails echanged with poi. It shows how intense are echanges with POI regardless of the volume of emails.   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagram](perc_from_poiVSperc_to_poi.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# creating new features: perc_to_poi and perc_from_poi\n",
    "create_new_features(data_dict,df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Missing values : do not include these features\n",
    "drop_features(df)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the data for Machine Learning algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naive Bayes, as well as Decision trees and Tree-based ensemble methods (RF, XGB) are invariant to feature scaling.\n",
    "\n",
    "Thus, I did not used feature scaling. \n",
    "\n",
    "Affected algorithms are for example: SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selection of features and use of SelectKBest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I'll also do feature selection using percentile and k-best algorithms, I want to see the top 9 features these algorithms will select"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagram](FeatureImportance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 13 selected features and after getting rid of correlated features. Here they are:\n",
    "- 'poi',\n",
    "- 'exercised_stock_options',\n",
    "- 'bonus',\n",
    "- 'salary',\n",
    "- 'perc_to_poi',\n",
    "- 'deferred_income',\n",
    "- 'long_term_incentive',\n",
    "- 'total_payments',\n",
    "- 'shared_receipt_with_poi',\n",
    "- 'expenses',\n",
    "- 'from_poi_to_this_person',\n",
    "- 'perc_from_poi',\n",
    "- 'from_this_person_to_poi',\n",
    "- 'from_messages'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# to make this notebook's output identical at every run\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# select best features and getting rid of correlated features: 13 are remaining in total\n",
    "features_list = ['poi',\n",
    "                'exercised_stock_options',\n",
    "                'bonus',\n",
    "                'salary',\n",
    "                'perc_to_poi',\n",
    "                'deferred_income',\n",
    "                'long_term_incentive',\n",
    "                'total_payments',\n",
    "                'shared_receipt_with_poi',\n",
    "                'expenses',\n",
    "                'from_poi_to_this_person',\n",
    "                'perc_from_poi',\n",
    "                'from_this_person_to_poi',\n",
    "                'from_messages']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Store to my_dataset for easy export below.\n",
    "my_dataset = data_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Extract features and labels from dataset for local testing\n",
    "data = featureFormat(my_dataset, features_list, sort_keys = True)\n",
    "labels, features = targetFeatureSplit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number train dataset: ', 127)\n",
      "('Number test dataset: ', 15)\n",
      "('Total number: ', 142)\n"
     ]
    }
   ],
   "source": [
    "### split data into training and testing datasets\n",
    "features_train, features_test, labels_train, labels_test = train_test_split(features, labels, test_size=0.1, random_state=42)\n",
    "\n",
    "print(\"Number train dataset: \", len(features_train))\n",
    "print(\"Number test dataset: \", len(features_test))\n",
    "print(\"Total number: \", len(features_train)+len(features_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 4: Try a varity of classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direction:\n",
    "Try a varity of classifiers\n",
    "\n",
    "Please name your classifier clf for easy export below.\n",
    "\n",
    "Note that if you want to do PCA or other multi-stage operations,\n",
    "you'll need to use Pipelines. For more info:\n",
    "http://scikit-learn.org/stable/modules/pipeline.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process is the following:\n",
    "1. Instantiate the model\n",
    "2. Train the model on training data\n",
    "3. Compute the performance (Accuracy, Precision, Recall, F1) using cross-validation.\n",
    "\n",
    "I tried the following algorithms and the results are:\n",
    "\n",
    "Algorithm|Accuracy|Precision|Recall|F1\n",
    "--|--|--|--|--|--\n",
    "NaiveBayes|0.83993|0.37073|0.28750|0.32385\n",
    "DecisionTree|0.81713|0.30701|0.29550|0.30115\n",
    "RandomForest|0.86327|0.44218|0.09750|0.15977\n",
    "AdaBoost|0.81860|0.31195|0.29900|0.30534\n",
    "\n",
    "They are all below expectation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 5: Tune your classifier to achieve better than .3 precision and recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Direction:\n",
    "Tune your classifier to achieve better than .3 precision and recall \n",
    "using our testing script. Check the tester.py script in the final project\n",
    "folder for details on the evaluation method, especially the test_classifier\n",
    "function. \n",
    "\n",
    "Because of the small size of the dataset, the script uses\n",
    "stratified shuffle split cross validation. \n",
    "For more info: \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.cross_validation.StratifiedShuffleSplit.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tuning consists in finding the algorithm parameters to get more accurate machine learning models on both training set and testing set. \n",
    "\n",
    "The model parameters are learned during training - e.g coefficients in linear regression - and we should not confuse model parameters with hyperparameters.  \n",
    "\n",
    "Hyperparameters are set by the Data Analyst before training the model.\n",
    "\n",
    "When there are many hyperparameters, tuning could be tedious. In this case, it is efficient - cost and time - to use automatic search such as GridSearchCV.\n",
    "\n",
    "It is important to fine-tune the model since we want the best performance (e.g. minimal errors, best accuracy, best precision, and so on). \n",
    "\n",
    "I selected F1 for performance scoring since it is a good balance between Recall and Precision.\n",
    "\n",
    "When tuning with one algorithm fails, there is an option to switch to other algorithms. \n",
    "\n",
    "__Naive Bayes tuning:__\n",
    "\n",
    "There is no hyperparameter tuning since the algorithm does not allow algorithm parameter tuning. \n",
    "\n",
    "To improve the performance, we can only play with the dataset - get more data - and the number of features. \n",
    "\n",
    "I did play with the number of features. \n",
    "\n",
    "I finally selected one feature: exercised_stock_options.\n",
    "\n",
    "The process followed is: \n",
    "1. set the feature list with the unique feature\n",
    "2. Instantiate the Naive Bayes model\n",
    "3. Train the Naive Bayes model with the training data\n",
    "4. Evaluation on the final Naive Bayes model\n",
    "\n",
    "\n",
    "__Decision Tree tuning:__\n",
    "\n",
    "Random Forest algorithm has a lot of hyperparameters and I decided to use the following ones: \n",
    "'criterion','max_depth','min_samples_split','max_features'. \n",
    "\n",
    "I also tried a smaller set of features to improve further the performance. The selection of the final features used the feature importances of the previous run Decision Tree model. \n",
    "\n",
    "Features giving the best results: 'exercised_stock_options','salary','perc_to_poi','deferred_income','total_payments','shared_receipt_with_poi','expenses','from_poi_to_this_person','from_this_person_to_poi'\n",
    "\n",
    "The process followed is: \n",
    "1. set the feature list with the list here above\n",
    "2. Tuning with the following hyperparameters: criterion, max_depth, min_samples_split, max_features\n",
    "3. Use of GridSearchCV and F1 as scoring\n",
    "4. reuse of best estimator\n",
    "5. Evaluation on the final Decision Tree model\n",
    "\n",
    "__Random Forest tuning:__\n",
    "\n",
    "Random Forest algorithm has a wealth of hyperparameters and I decided to use the following ones: 'criterion','n_estimators','max_depth','max_features'. \n",
    "\n",
    "I also tried a smaller set of features to improve further the performance. The selection of the final features used the feature importances from the previous run of Random Forest. \n",
    "\n",
    "I finally selected two features: 'exercised_stock_options','bonus'\n",
    "\n",
    "The process followed is: \n",
    "1. set the feature list with the list here above\n",
    "2. Tuning with the following hyperparameters: criterion, n_estimators, max_depth, max_features\n",
    "3. Use of GridSearchCV and F1 as scoring\n",
    "4. reuse of best estimator\n",
    "5. Evaluation on the final Random Forest model\n",
    "\n",
    "__Adaboost tuning:__\n",
    "\n",
    "Adaboost algorithm has a wealth of hyperparameters and I decided to use the following ones:'n_estimators','algorithm','learning_rate'. \n",
    "\n",
    "I also tried a smaller set of features to improve further the performance. The selection of the final features used the feature importances from previous run of Adaboost.\n",
    "\n",
    "Final features giving the best results: 'exercised_stock_options','perc_to_poi','long_term_incentive','total_payments','shared_receipt_with_poi','expenses','from_this_person_to_poi'\n",
    "\n",
    "The process followed is: \n",
    "1. set the feature list with the list here above\n",
    "2. Tuning with the following hyperparameters: n_estimators,algorithm,learning_rate\n",
    "3. Use of GridSearchCV and F1 as scoring\n",
    "4. reuse of best estimator\n",
    "5. Evaluation on the final adaboost model\n",
    "\n",
    "Algorithm|Accuracy|Precision|Recall|F1| pass/fail\n",
    "--|--|--|--|--|--|--\n",
    "Naive Bayes|0.90409|0.46055|0.32100|0.3783|passed\n",
    "Decision Tree|0.83993|0.41938|0.52150|0.46490|passed\n",
    "Random Forest|0.81754|0.40663|0.40500|0.40581|passed\n",
    "AdaBoost|0.87213|0.51576|0.67100|0.58322| passed "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DecisionTreeClassifier visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Diagram](DecisionTree02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Direction \n",
    "\n",
    "Dump your classifier, dataset, and features_list so anyone can\n",
    "check your results. You do not need to change anything below, but make sure\n",
    "that the version of poi_id.py that you submit can be run on its own and\n",
    "generates the necessary .pkl files for validating your results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See: poi_id.py file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Usage of Evaluation Metrics__\n",
    "\n",
    "In case of classification problems and when the dataset is unbalanced - i.e. a difference in the numbers of positive and negative instances, usually with the negatives outnumbering the positives - the most appropriate metric is not accuracy but some alternate metrics (precision, recall).\n",
    "\n",
    "Accuracy in classification problems is the number of correct predictions made by the model over all kinds predictions made.\n",
    "\n",
    "The meaning of precision is the following: a precision of 0.3 means that there are 30% correct predictions among the positive predictions. Precision is a measure that tells us what proportion of people that we predicted as being poi, actually are poi.\n",
    "\n",
    "Recall is a measure that tells us what proportion of people that actually are poi was predicted by the algorithm as being poi.\n",
    "\n",
    "Here is an example with RandomForest Classifier with an imbalanced dataset:\n",
    "\n",
    "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='entropy',\n",
    "            max_depth=None, max_features=1, max_leaf_nodes=None,\n",
    "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
    "            min_samples_leaf=1, min_samples_split=2,\n",
    "            min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
    "            oob_score=False, random_state=42, verbose=0, warm_start=False)\n",
    "            \n",
    "__Accuracy: 0.86327\tPrecision: 0.44218\tRecall: 0.09750\tF1: 0.15977\tF2: 0.11551__\n",
    "\n",
    "__Total predictions: 15000\tTrue positives:  195\tFalse positives:  246\tFalse negatives: 1805\tTrue negatives: 12754__\n",
    "\n",
    "The key point is that accuracy metric fails to capture the poor performance of the classifier for the imbalanced dataset. For example, accuracy indicates that the performance of the classifier is fine with 0.86. But the recall (respectively precision)  indicates that the performance of the classifier is relatively poor (respectively acceptable) on the imbalanced dataset with 0.10 (respectively with 0.44). Hence, precision and recall reveal differences in performance that go unnoticed when using accuracy.\n",
    "\n",
    "For GridSearchCV, I use F1 as scoring since the recall and the precision are equally important.\n",
    "\n",
    "__Validation and its importance__\n",
    "\n",
    "Validation is to ensure that the model could generalise well. For that purpose the dataset is to split the data into 2 sets: training set and test set. \n",
    "\n",
    "Cross-validation was used here. The concept is the following: Instead of using the whole dataset to train and then test on same data, I randomly divide our data into training and testing datasets.\n",
    "\n",
    "If validation is not performed correctly, when the model is deployed into production it is highly probable that with new data the performance will drop since the model could not generalise well.\n",
    "\n",
    "__Algorithm Performance__\n",
    "\n",
    "The algorithm is validated when precision and recall are both at least 0.3\n",
    "\n",
    "Algorithm|Accuracy|Precision|Recall|F1| Comment\n",
    "--|--|--|--|--|--|--\n",
    "Naive Bayes|0.90409|0.46055|0.32100|0.3783|\n",
    "Decision Tree|0.83993|0.41938|0.52150|0.46490|\n",
    "Random Forest|0.81754|0.40663|0.40500|0.40581|\n",
    "AdaBoost|0.87213|0.51576|0.67100|0.58322| Selected since Best F1 \n",
    "\n",
    "When tester.py is used to evaluate performance, precision and recall are both at least 0.3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is very small. And data matters as well as algorithm.\n",
    "\n",
    "Three Outliers have been removed with significant impact.\n",
    "\n",
    "New features have been added (e.g. perc_to_poi) and for some models it was efficient. The feature perc_to_poi is the second highest significant feature for Adaboost Model (selected model). The feature perc_to_poi is highly significant for the tuned Decision Tree model.\n",
    "\n",
    "Relevant data have been used. When using all the features, models performed poorly because of overfitting in some cases. A good approach was to use a smaller set of features.\n",
    "\n",
    "There was no way to have more training data.\n",
    "\n",
    "The key influencing features are financial ones (e.g. exercised_stock_options)\n",
    "\n",
    "Some features have very few information: loan_advances, director_fees, restricted_stock_deferred. The approach was to not include them in feature selection.\n",
    "\n",
    "Tuning could be time consuming and specially if done manually. It's why the use of GridSearch helped a lot.\n",
    "\n",
    "The selected model - the one with the best F1 - is AdaBoost with the following scores:\n",
    "\n",
    "Accuracy: 0.87213\t\n",
    "\n",
    "Precision: 0.51576\t\n",
    "\n",
    "Recall: 0.67100\t\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation/References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GitHub repository 1]\n",
    "\n",
    "https://github.com/ageron/handson-ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GitHub repository 2]\n",
    "\n",
    "https://github.com/MarcCollado/Enron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GitHub repository 3]\n",
    "\n",
    "https://github.com/travisseal/enron_data_udacity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GitHub repository 4]\n",
    "\n",
    "https://github.com/Jacobdudee/EnronModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GitHub repository 5]\n",
    "\n",
    "https://github.com/adazamora/enron_ml/blob/master/ml_project.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[GitHub repository 6]\n",
    "\n",
    "https://github.com/WillKoehrsen/Machine-Learning-Projects/tree/master/random_forest_explained"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Hdbk] Python for Data Science Handbook from Blog:\n",
    "[blog](http://www.datasciencecentral.com/profiles/blogs/book-python-data-science-handbook?utm_content=buffer09a5c&utm_medium=social&utm_source=twitter.com&utm_campaign=buffer)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
